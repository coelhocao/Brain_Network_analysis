#############################################################################################
# These codes have been costumly designed for brain network analysis of 1-2 time point data
# generally acquired by immunolabelling of activity-dependent proteins (i.e. c-fos, Arc)
#
# Many functions here have been largely based on some of the codes shared by Justin Kenney,
# which can be found on this link https://github.com/jkenney9a/Networks
#
# NOTE: It is intended mostly for undirected network data
# Most funcitons won't work in structural brain data or networks which support directed graphs 
# or different upper/lower matrix triangles.
#
# NOTE: Designed for comparisons between single-time point graphs or two-time points graphs
# won't work on multiple time points graphs
#
# Cesar A O Coelho
# cebacio@gmail.com
#

require (igraph) #package for network generation and measures
require(Hmisc) #better functions for correlation matrices and p-values
require (lattice) #for generating graphs, colors and matrices

setwd(file.path("C:/Users/Cesar/Dropbox/R")) #Set working directory

#Loads data in either .csv or .txt files
load_data <- function(file){
  #   Input: either a .txt or a .csv or a excel file name
  #   
  #   Output: Dataframe containing the file
  
  if(tools::file_ext(file) == "txt"){
    d <- read.table(file=file, header=TRUE, check.names=FALSE)
  }
  else{
    if(tools::file_ext(file) == "csv"){
      d <- read.table(file=file, sep = ",", header=TRUE, check.names=FALSE)
    }
  }
  
  return(d)
}


clean_data <- function(df, real_zeros=TRUE, missing_thresh= 0.1, fill_missing=TRUE){
  # Input: data as loaded
  # 
  #removes undesired zeros, remove columns with too many missing data, fill missing data with mean of column
  #
  # Output: cleaned data
  
  #if desired, Replace zeros in data with NA
  if (real_zeros==FALSE){
    df[mapply("==", df, 0)] <- NA
  }
  
  #Remove columns with more than missing_thresh missing values
  df_out <- df[,which(colSums(is.na(df))/dim(df)[1] < missing_thresh)]
  
  #Fills missing values with mean column value
  #(doesn't work if cleaned data doesn't have missing data to be filled)
  if (fill_missing==TRUE){
    col_avgs <- colMeans(df_out, na.rm=TRUE)
    index <- which(is.na(df_out), arr.ind=TRUE)
    df_out[index] <- col_avgs[index[,2]]
  }
  
  return(df_out)
}


corr_matrix <- function(df, p.adjust.method='none', type='pearson'){
  #
  # Input: Dataframe
  #
  # Whether or not to apply a p-value adjustment method drawn from the 'p.adjust' function
  # (e.g, 'fdr', 'bonferroni' etc.). 
  # whether to use 'pearson' or 'spearman' correlation
  # NOTE: assume undirected graph!
  #   
  # Output: List of two dataframes corresponding to 1) all pairwise Pearson 
  # correlations and 2) all associated p-values, adjusted or not(default)
  

  corr <- as.data.frame(rcorr(as.matrix(df), type=type)$r)
  
  #adjust p-values if necessary
  p_values <- as.matrix(rcorr(as.matrix(df), type=type)$P)
  p_values[lower.tri(p_values)] <- p.adjust(p_values[lower.tri(p_values)], method=p.adjust.method) 
  names(p_values[])=colnames(corr)
  p_values <- as.data.frame(p_values)
  

  
  return(list("corr" = corr,"pvalue" = p_values))
}


corr_matrix_threshold <- function(df, negs = 'zero', thresh=0.01, thresh.param='p', p.adjust.method='none', type='pearson'){
  #
  # Input: Dataframe with headers as titles (brain regions)
  #
  # removes or normalize negative correlations:
  # zero (set to zero, default), abs (absolute value), sqrt (squareroot), resc (rescale to 0-2), or none
  # set threshold value AND parameter (r, p, or cost), the p-value adjustment method to use (if any),
  # the type of correlation (pearson or spearman).
  #
  # IMPORTANT: p.adjust.method WILL modify your p_value matrix used for thresh/thresh.param. BE AWARE!!
  #   
  # Output: Dataframe of correlations thresholded at p < threshold
  
  dfs <- corr_matrix(df, p.adjust.method=p.adjust.method, type=type)

  
  #remove any NaNs, infs or NAs (sometimes happens with bootstrapping; not sure why)
  dfs$pvalue[mapply(is.infinite, dfs$corr)] <- 1
  dfs$pvalue[mapply(is.nan, dfs$corr)] <- 1  
  dfs$pvalue[mapply(is.na, dfs$pvalue)] <- 1
  dfs$corr[mapply(is.infinite, dfs$corr)] <- 0
  dfs$corr[mapply(is.nan, dfs$corr)] <- 0
  dfs$corr[mapply(is.na, dfs$corr)] <- 0
  
  #remove negative correlations
  if(negs == 'zero'){
    dfs$corr[mapply("<", dfs$corr, 0)] <- 0
  }
  else{
    if(negs == 'abs'){
      dfs$corr <- abs(dfs$corr)
    }
    else{
      if(negs == 'sqrt'){
        dfs$corr <- sqrt(dfs$corr)
      }
      else{
        if(negs == 'resc'){
          dfs$corr <- 1+ dfs$corr
        }
        else{
          if(negs == 'none'){
          dfs$corr <- dfs$corr}
        }
      }
    }
  }

  
  if(tolower(thresh.param)=='p'){
    #apply p-value threshold to correlation matrix
    dfs$corr[mapply(">=", dfs$pvalue, thresh)] <- 0    
  } else if(tolower(thresh.param)=='r'){
    dfs$corr[mapply("<=", abs(dfs$corr), thresh)] <- 0
  } else if(tolower(thresh.param)=='cost'){
    r.threshold <- quantile(abs(dfs$corr), probs=1-thresh, na.rm=TRUE)
    dfs$corr[mapply("<=", abs(dfs$corr), r.threshold)] <- 0
  } else{
    stop("Invalid thresholding parameter")
  }
  
  return(dfs$corr)
}




file_to_graph <- function(file, clean_data = '', negs = 'zero', thresh=0.01, thresh.param='p', p.adjust.method='none',
                          type='pearson', weighted = TRUE){
  #
  #Input: csv, txt file
  #
  # cleans the data, removes or normalize negative correlations:
  # zero (set to zero, default), abs (absolute value), sqrt (squareroot), resc (rescale to 0-2), or none
  # set threshold value AND parameter (r, p, or cost), the p-value adjustment method to use (if any),
  # the type of correlation (pearson or spearman) and if graph should be waited or not.
  #
  # Output: igraph graph
  
  df <- load_data(file)
  if(clean_data == 'yes'){
    df <- clean_data(df)
  }
  
  df_G <- corr_matrix_threshold(df, negs = negs, thresh=thresh, thresh.param=thresh.param, 
                                p.adjust.method=p.adjust.method, type=type)
  
  G <- graph.adjacency(as.matrix(abs(df_G)), mode="undirected", weighted=weighted)
  
  return(G)
}


df_to_graph <- function(df, clean_data='', negs='zero', thresh=0.01, thresh.param='p', p.adjust.method='none', type='pearson', weighted=TRUE){
  #
  # Input: dataframe var
  #
  # cleans the data, removes or normalize negative correlations:
  # zero (set to zero, default), abs (absolute value), sqrt (squareroot), resc (rescale to 0-2), or none
  # set threshold value AND parameter (r, p, or cost), the p-value adjustment method to use (if any),
  # the type of correlation (pearson or spearman) and if graph should be waited or not.
  #
  # Output: igraph graph
 
  df <-as.data.frame(df)
  
   if(clean_data == 'yes'){
    df <- clean_data(df)
  }
  
  df_G <- corr_matrix_threshold(df, negs = negs, thresh=thresh, thresh.param=thresh.param, 
                                p.adjust.method=p.adjust.method, type=type)
  G <- graph.adjacency(as.matrix(abs(df_G)), mode="undirected", weighted=weighted)
  
  return(G)
}


Nodal_measures <-function(G, normalized=FALSE, efficiency_loss= FALSE, local_eff=FALSE, min_max_normalization=FALSE){
  # Input: An igraph graph
  #
  # Output: A dataframe of centrality measures:
  # degree, strength, betweenness, eigenvector, closeness, Burt's constraint, leverage,
  # transitivity (~clustering coefficient), nodal efficiency,
  #
  # Optional: local efficiency, loss of efficiency (global and mean local) after region removal
  # provides how much the network loses of Gl and Lc efficiency by losing a region (different than nodal efficiency)
  # HOWEVER, this calculation takes a really long time. That's why default is FALSE
  #
  # NOTE: nodal efficiency makes use of weights if they exist. For unweighted measures, create unweighted graphs!!
  # Optional: normalize all values according to min and max values
  #
  #
  
  if(ecount(G) == 0){
    zeros <- rep(0, vcount(G))
    output <- as.data.frame(cbind("degree"=zeros, "strength"=zeros, "eigenvector"=zeros,"betweenness"=zeros, 
                                  "burt"=zeros,"closeness"=zeros, "leverage" = zeros, "transitivity"=zeros,
                                  "Nodal_eff" = zeros), row.names=V(G)$name)
    if(local_eff == TRUE){output$local_eff = zeros}
    if(efficiency_loss == TRUE){
    output$geff_loss = zeros 
    output$leff_loss = zeros
    }
    return(output)
  }
  
  
  G.pos <- G
  E(G.pos)$weight <- abs(E(G.pos)$weight) #Positive numbers are necessary for betweenness and closeness
  E(G.pos)$weight <- 1-E(G.pos)$weight #bet and clo are based on distance graphs, not weights. here, dist= 1 - corr
  
  #Centrality measures
  dg <- igraph::degree(G, normalized=normalized)
  stg <- strength(G)
  evc <- evcent(G)$vector
  bet <- betweenness(G.pos, normalized=normalized)
  clo <- closeness(G.pos, normalized=normalized)
  burt <- constraint(G)
  lev <- leverage(G)
  trans <- transitivity(G,type="local", isolates='zero')
  ndeff <- Nodal_efficiency(G, normalized = normalized)
  
  #when degree = 0, burt = NaN. Set it to a value higher than max since lower values indicate more centrality
  burt[!is.finite(burt)] <- max(!is.finite(burt)) + 0.1 
  
  output <- data.frame("degree" = dg, "strength" = stg, "eigenvector" = evc, "betweenness" = bet,"burt" = burt,
                       "closeness" = clo, "leverage" = lev, "transitivity" = trans, "Nodal_eff" = ndeff)
  
  # include nodal local eff (different from nodal_eff) in the dataframe
  if(local_eff == TRUE){
    
  leff <- local_eff(G)
  output$local_eff <- leff
  }
  
  #include loss of efficiency (global and local) after that node's removal (different from nodal_eff and local_eff) 
  if(efficiency_loss == TRUE){
    
    loss <- efficiency_loss(G)
    output$geff_loss <- loss[,1]
    output$leff_loss <- loss[,2]
  }
  
  if(min_max_normalization==TRUE){
    output <- apply(output, MARGIN=2, function(x) {(x-min(x)) / (max(x) - min(x))})
    output <- as.data.frame(output)
    output$transitivity <- trans #Do not normalize transitivity b/c it is already normalized
    output$eigenvector <- evc #eigenvector is also a normalized index ranging from 0-1
    output$leverage <- lev #leverage is also normalized from -1 to 1.
  }
  
  return(output)
}


leverage <- function(g){
  #
  # Input: igraph graph
  #
  # Calculates centrality as defined in  
  # Joyce et al (2010). A New Measure of Centrality for Brain Networks (PLoS ONE 5(8):e12200)
  # code mostly taken from igraph wiki igraph.wikidot.com/r-recipes#toc10
  #
  # Output: Leverage centrality
  #
  
  k <- degree(g)
  n <- vcount(g)
  
  lev <- sapply(1:n, function(v) { mean((k[v]-k[neighbors(g,v)]) / (k[v]+k[neighbors(g,v)])) })
  
  #when k = 0, leverage = NA or NaN. As this can compromise permutations,
  #and both negative and positive values are meaningful, the closest interpretation is -1
  lev[lev %in% NA] <- -1
  lev[lev %in% NaN] <- -1
  names(lev) <- V(net0)$name
  
  return(lev) 
}


Nodal_efficiency <- function(G, normalized=FALSE) {
  #Input: igraph graph, weights will be used if existent
  # whether or not to normalize (where max value = 1)
  
  #Output: data frame of nodal efficiency (i.e, average inverse shortest path length from individual node to all other nodes)
  
  if(ecount(G) > 0){
    E(G)$weight <- 1 - abs(E(G)$weight) #calculate distance matrix from correlation matrix b/c
  }
  eff <- 1 / distances(G)
  
  eff[!is.finite(eff)] <- 0
  out <- colSums(eff) / (vcount(G) - 1)
  
  if(normalized == TRUE){
    out <- out / max(out)
  }
  return(out)
}



gl_eff <- function(g, weights=TRUE) {
  #
  # Input: igraph graph
  #
  # calculates the average inverse shortest paths between all u--v vertices
  # If weights=FALSE, value should range from 0-1, if weights=TRUE it can go above 1.
  # distances are the opposite of correlation. Therefore distance is treated here as 1 - corr (weight)
  #
  # Output: a value of Global Efficiency
  #
  
  E(g)$weight<- 1 - E(g)$weight
  if(weights == FALSE){
    eff <- 1/distances(g, weights= NA)} #inverse of shortest paths for every i-j node pair
  else{
    eff <- 1/distances(g)
  }
  
  eff[!is.finite(eff)]<-0 
  eff<-mean(eff[lower.tri(eff)],na.rm=TRUE) #mean of the inverse shortest paths btwn every i-j node pair
  return(eff) 
}


local_eff <- function(g) {
  #
  # Input: igraph graph
  # Calculates efficiency for a subgraph composed of all neighbors of a given node
  # all obs in gl_eff are true here
  #
  # Output: a vector with nodal local efficiency
  #
  
  eff <- numeric(length(V(g)))
  nodes <- V(g)$name
  eff <- simplify2array(sapply(nodes, function (x){
    neigbs <- neighbors(g,v=x)
    subg <- induced.subgraph(g,neigbs)
    eff <- gl_eff(subg)
  }))

  eff[eff %in% NaN]<-0
  return(eff)
}


efficiency_loss <- function(g){
  # Input: igraph graph
  #
  # removes a node, calculate the Global and mean local efficiency relative to original graph's values
  # NOTE: takes a LONG TIME. it seems it's not optimized. Better not use with resampling procedures.
  #
  # Output: dataframe with the above calculation for each region
  
  rem <- function(net,r) {
    # Remove vids vertices by random
    vids<-V(net)[r]
    net <- delete.vertices(net,vids)
    return(net)
  }
  
  rem_calc <- function(g,r){
    
    
    gr <- rem(g,r)
    grgef <- gl_eff(gr)
    grlef <- mean(local_eff(gr))
    
    
    output <- c(grgef, grlef)
    return(output)
  }
  
  m <- as.data.frame(t(sapply (1:vcount(g), FUN= function (x) rem_calc(g,x))))
  geg <- gl_eff(g)
  leg <- mean(local_eff(g))
  if(geg == 0){ m[,1] <- rep(0, length(m[,1]))}
  else{
    m[,1] <- 1 - (m[,1]/geg)
  }
  if(leg == 0){ m[,2] <- rep(0, length(m[,1]))}
  else{
    m[,2] <- 1 - (m[,2]/leg)
  }
  colnames(m) <- c("gleff0/gleffi", "leff0/leffi")
  
  return(m)
}


nodal_permutation_test <- function(df, labels, seed=1235, resample=10000, negs='zero', thresh=0.05, thresh.param='p', p.adjust.method='none', type='pearson', weighted=TRUE,
                             normalized=FALSE, efficiency_loss= FALSE, local_eff=FALSE, min_max_normalization=FALSE){
  #
  # Input: dataframe, grouping labels and resampling number
  #
  # resample labels, regenerate graphs, calculate network measures, group diffs among them
  # generate empirical networks, calculate measures and group differences
  # calculate the proportion of resampled diffs > empirical differences (p-value)
  #
  # NOTE: The parameters to be put are both for the empirical and resampled graphs
  # NOTE: The function accepts multiple groups, but Pvalue is not corrected for multiple comparisons
  #
  # Output: 
  #
  
  if(length(levels(labels)) < 2)
  {"ERROR! It's a between network design. Requires at least TWO labels necessary to compare"}
  

  resamp <- function(df, labels,negs=negs, thresh=thresh, thresh.param=thresh.param, 
                     p.adjust.method=p.adjust.method, type=type, weighted=weighted,normalized=normalized, efficiency_loss= efficiency_loss, 
                     local_eff=local_eff, min_max_normalization=min_max_normalization){
    #shuffle the grouping labels
    newlabels <- factor(sample(labels, replace=F))
    
    #randomized networks
    nets <- list()
    nets <- lapply(seq_along(levels(newlabels)), function(x){
      nets$x <- Nodal_measures(df_to_graph(df[which(newlabels == levels(newlabels)[x]),], negs=negs, thresh=thresh, thresh.param=thresh.param, 
                                           p.adjust.method=p.adjust.method, type=type, weighted=weighted), normalized = normalized, 
                               efficiency_loss = efficiency_loss, local_eff = local_eff, min_max_normalization = min_max_normalization)
    })
    
    names(nets) <- levels(newlabels)

    diff_label <- combn(names(nets), m=2, FUN = paste0)
    diffs <- list()
    comb <- combn(nets,2, simplify = FALSE)
    diffs <- lapply(comb, function(x) abs(x[[1]] - x[[2]]))
    
    dim_names <- list()
    dim_names$dim1 <- rownames(nets[[1]])
    dim_names$dim2 <- names(diffs[[1]])
    dim_names$dim3 <- sapply(1:ncol(diff_label), function(x){ paste(diff_label[,x], collapse = "-")})
    diffs <- array(as.numeric(unlist(diffs)), dim=c(length(diffs[[1]][,1]), length(diffs[[1]]), length(diffs)), 
                  dimnames = dim_names)
    
    return(diffs)
  }
  
  set.seed(seed)
  resamp_distrib <- replicate(resample, resamp(df,labels,negs=negs, thresh=thresh, thresh.param=thresh.param,
                                         p.adjust.method=p.adjust.method, type=type, weighted=weighted,
                                         normalized=normalized,efficiency_loss= efficiency_loss,
                                         local_eff=local_eff, min_max_normalization=min_max_normalization))
  
  #empirical networks generation and differences
  emp_measures <- list()
  emp_measures <- lapply(seq_along(levels(labels)), function(x){
    emp_measures$x <- Nodal_measures(df_to_graph(df[which(labels == levels(labels)[x]),], negs=negs, thresh=thresh, thresh.param=thresh.param, 
                                         p.adjust.method=p.adjust.method, type=type, weighted=weighted), normalized = normalized, 
                             efficiency_loss = efficiency_loss, local_eff = local_eff, min_max_normalization = min_max_normalization)
  })
  
  names(emp_measures) <- levels(labels)
  emp_diff_label <- combn(names(emp_measures),m=2, FUN = paste0)
  emp_diffs <- list()
  emp_comb <- combn(emp_measures,2, simplify = FALSE)
  emp_diffs <- lapply(emp_comb, function(x) abs(x[[1]] - x[[2]]))
  
  dim_names <- list()
  dim_names$dim1 <- rownames(emp_measures[[1]])
  dim_names$dim2 <- names(emp_measures[[1]])
  dim_names$dim3 <- sapply(1:ncol(emp_diff_label), function(x){ paste(emp_diff_label[,x], collapse = "-")})
  
  emp_diffs <- array(as.numeric(unlist(emp_diffs)), dim=c(length(emp_diffs[[1]][,1]), length(emp_diffs[[1]]), length(emp_diffs)), 
                 dimnames = dim_names)
  
  #Calculating the p-values
  ps <- array(0, dim = dim(emp_diffs))

  for(x in 1:dim(emp_diffs)[3]){
    for(y in 1:dim(emp_diffs)[2]){
      for(z in 1:dim(emp_diffs)[1]){
        ps[z,y,x] <- mean(abs(resamp_distrib[z,y,x,]) > abs(emp_diffs[z,y,x]))
      }
    }
  }
  
  dimnames(ps) <- dimnames(emp_diffs)
  
  return(list(emp_diffs,ps))
              
}


## A little testing on the permutation test to assure no ERROR type I will be made is suggested
